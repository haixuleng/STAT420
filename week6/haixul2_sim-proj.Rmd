---
title: 'Week 6 - Midterm Assignment: A Simulation Project'
author: "STAT 420, Summer 2021, haixul2"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```



```{r}
birthday = 19900210
set.seed(birthday)
```

# Simulation Study 1: Significance of Regression

## Introduction

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

Use simulation to obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

For each model and $\sigma$ combination, use $2000$ simulations. For each simulation, fit a regression model of the same form used to perform the simulation.

Use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Done correctly, you will have simulated the `y` vector $2 (models)×3 (sigmas)×2000 (sims)=12000$ times.

Potential discussions:

- Do we know the true distribution of any of these values?
- How do the empirical distributions from the simulations compare to the true distributions? (You could consider adding a curve for the true distributions if you know them.)
- How are each of the $F$ statistic, the p-value, and $R^2$ related to $\sigma$? Are any of those relationships the same for the significant and non-significant models?

An additional tip:

- Organize the plots in a grid for easy comparison. For example, a $1 × 3$ row of $F$ statistic plots as $\sigma$ changes, then a $1 × 3$ row of $p$-value plots as $\sigma$ changes, followed by a similar row for the $R^2$ values. Consider a similar setup for the values attributed to the significant model and then again for the nonsignificant model.

## Methods

### Build a function to simulate results

```{r}
# define a function that takes beta, sigma and X as input, and output a vector of y
sim_model = function(beta = c(3, 1, 1, 1), sigma = 1, X = read.csv("study_1.csv")){
  X_m = as.matrix(X)
  X_m[, 1] = rep(1, length(X_m[, 1]))
  X[, 1] = X_m %*% beta + rnorm(length(X[, 1]), mean = 0, sd = sigma)
  (X)
}
```

### Simulation: 

```{r}
N = 2000
f_vec = rep(0, N * 6) # 6 = 2 models x 3 sigmas
p_vec = rep(0, N * 6)
r_squared_vec = rep(0, N * 6)
index = 1

# significant model
beta_sim = c(3, 1, 1, 1)
for (sigma_sim in c(1, 5, 10)){
  for (i in 1:N){
    Y = sim_model(beta = beta_sim, sigma = sigma_sim)
    model_sim = lm(y ~ x1 + x2 + x3, data = Y)
    f_vec[index] = summary(model_sim)$fstatistic[1]
    p_vec[index] = pf(f_vec[index], summary(model_sim)$fstatistic[2], summary(model_sim)$fstatistic[3], lower.tail = FALSE)
    r_squared_vec[index] = summary(model_sim)$r.squared
    index = index + 1
  }
}

# non-significant model
beta_sim = c(3, 0, 0, 0)
for (sigma_sim in c(1, 5, 10)){
  for (i in 1:N){
    Y = sim_model(beta = beta_sim, sigma = sigma_sim)
    model_sim = lm(y ~ x1 + x2 + x3, data = Y)
    f_vec[index] = summary(model_sim)$fstatistic[1]
    p_vec[index] = pf(f_vec[index], summary(model_sim)$fstatistic[2], summary(model_sim)$fstatistic[3], lower.tail = FALSE)
    r_squared_vec[index] = summary(model_sim)$r.squared
    index = index + 1
  }
}
```

## Results

### Plot the distribution

**Distribution of F values of simulations with different models and different $sigma$ values.**

```{r}
# 3 sigmas of one model are in one row
par(mfrow=c(2,3))
hist(f_vec[1 : N], main = expression("Sig:" ~ sigma ~ "= 1"), xlab = "F values")
hist(f_vec[(N + 1) : (2 * N)], main = expression("Sig:" ~ sigma ~ "= 5"), xlab = "F values")
hist(f_vec[(2 * N + 1) : (3 * N)], main = expression("Sig:" ~ sigma ~ "= 10"), xlab = "F values")
hist(f_vec[(3 * N + 1) : (4 * N)], main = expression("Non:" ~ sigma ~ "= 1"), xlab = "F values")
hist(f_vec[(4 * N + 1) : (5 * N)], main = expression("Non:" ~ sigma ~ "= 5"), xlab = "F values")
hist(f_vec[(5 * N + 1) : (6 * N)], main = expression("Non:" ~ sigma ~ "= 10"), xlab = "F values")
```

**Distribution of P values of simulations with different models and different $sigma$ values.**

```{r}
# 3 sigmas are in one row
par(mfrow=c(2,3))
hist(p_vec[1 : N], main = expression("Sig:" ~ sigma ~ "= 1"), xlab = "P values")
hist(p_vec[(N + 1) : (2 * N)], main = expression("Sig:" ~ sigma ~ "= 5"), xlab = "P values")
hist(p_vec[(2 * N + 1) : (3 * N)], main = expression("Sig:" ~ sigma ~ "= 10"), xlab = "P values")
hist(p_vec[(3 * N + 1) : (4 * N)], main = expression("Non:" ~ sigma ~ "= 1"), xlab = "P values")
hist(p_vec[(4 * N + 1) : (5 * N)], main = expression("Non:" ~ sigma ~ "= 5"), xlab = "P values")
hist(p_vec[(5 * N + 1) : (6 * N)], main = expression("Non:" ~ sigma ~ "= 10"), xlab = "P values")
```

**Distribution of $R^2$ of simulations with different models and different $sigma$ values.**

```{r}
# 3 sigmas are in one row
par(mfrow=c(2,3))
hist(r_squared_vec[1 : N], main = expression("Sig:" ~ sigma ~ "= 1"), xlab = expression(R^2 ~ " values"))
hist(r_squared_vec[(N + 1) : (2 * N)], main = expression("Sig:" ~ sigma ~ "= 5"), xlab = expression(R^2 ~ " values"))
hist(r_squared_vec[(2 * N + 1) : (3 * N)], main = expression("Sig:" ~ sigma ~ "= 10"), xlab = expression(R^2 ~ " values"))
hist(r_squared_vec[(3 * N + 1) : (4 * N)], main = expression("Non:" ~ sigma ~ "= 1"), xlab = expression(R^2 ~ " values"))
hist(r_squared_vec[(4 * N + 1) : (5 * N)], main = expression("Non:" ~ sigma ~ "= 5"), xlab = expression(R^2 ~ " values"))
hist(r_squared_vec[(5 * N + 1) : (6 * N)], main = expression("Non:" ~ sigma ~ "= 10"), xlab = expression(R^2 ~ " values"))
```

## Discussion

### True distribution of F, P and $R^2$

  - Under the null hypothesis, the F value should follow the **F distribution** ($D(F)$) with $p = 4$ and $n = 25$, so the degree of freedom are 3 and 21. Under the alternative hypothesis, the F value should follow a **"non-central" F distribution**, which is shifted to the right.

  - Under the null hypothesis, The P value should follow a **uniform distribution**. As a reminder, P value is an integral of F distribution given the F value. For example, $P = \int_{F}^{\infty}D(F)dF$
  
  - According to my online research, under the null hypothesis the $R^2$ should follow a **Beta distribution**. The $R^2$ can be derived from F.
    
    \[
      F = \frac{SSR_{eg} / (p -1)}{SSE / (n - p)}
    \]
    
    \[
      F = \frac{SSR_{eg} / (p -1)}{(SST - SSR_{eg} ) / (n - p)}
    \]
    
    \[
      F = \frac{R^2 / (p -1)}{(1 - R^2) / (n - p)}
    \]
    
    \[
      R^2 = \frac{F}{F + \frac{n - p}{p - 1}}
    \]
    
### Comparison between empirical distribution and true distribution

**Distribution of F statistic under null hypothesis**

From the overlay of the F statistic distribution with empirical distribution we can conclude:

  - The distribution of F values from the **significant** model with $\sigma = 1$ looks very different from the null F distribution. This makes sense, as the data from the **significant** model should not be similar to the non-significant model. As $\sigma$ increases, the empirical distribution looks more like the true null F distribution.
  
  - The distribution of the **non-significant** model with $\sigma = 1$ looks the closest to the true null F distribution. This observation is expected. As $\sigma$ increases, the distribution stays the same.
  
  

```{r}
par(mfrow=c(2,3))
x = seq(0, 10, 0.1)
p = 3 + 1
n = 25

hist(f_vec[1 : N], main = expression("Sig:" ~ sigma ~ "= 1"), xlab = "F values")
lines(x, df(x, p - 1, n - p) * N, col = "dodgerblue", lwd = 3, lty = 2)
hist(f_vec[(N + 1) : (2 * N)], main = expression("Sig:" ~ sigma ~ "= 5"), xlab = "F values")
lines(x, df(x, p - 1, n - p) * N, col = "dodgerblue", lwd = 3, lty = 2)
hist(f_vec[(2 * N + 1) : (3 * N)], main = expression("Sig:" ~ sigma ~ "= 10"), xlab = "F values")
lines(x, df(x, p - 1, n - p) * N, col = "dodgerblue", lwd = 3, lty = 2)
hist(f_vec[(3 * N + 1) : (4 * N)], main = expression("Non:" ~ sigma ~ "= 1"), xlab = "F values")
lines(x, df(x, p - 1, n - p) * N, col = "dodgerblue", lwd = 3, lty = 2)
hist(f_vec[(4 * N + 1) : (5 * N)], main = expression("Non:" ~ sigma ~ "= 5"), xlab = "F values")
lines(x, df(x, p - 1, n - p) * N, col = "dodgerblue", lwd = 3, lty = 2)
hist(f_vec[(5 * N + 1) : (6 * N)], main = expression("Non:" ~ sigma ~ "= 10"), xlab = "F values")
lines(x, df(x, p - 1, n - p) * N, col = "dodgerblue", lwd = 3, lty = 2)
```

**Distribution of P values under null hypothesis**

The P value of the null hypothesis should follow a uniform distribution by checking the distribution of the **non-significant** model with $\sigma = 1$. I am not able to find a derivation for this distribution.

  - The P values of the **significant** model with $\sigma = 1$ does not follow a uniform distribution. As $\sigma$ increases, the distribution approaches a uniform distribution.
  
  - The P values of the **non-significant** model with $\sigma = 1$ follows a uniform distribution. As $\sigma$ increases, the distribution is still quite close to a uniform distribution.

```{r}
par(mfrow=c(2,3))
x = seq(0, 1, 0.01)
y = rep(N / 10, length(x))
hist(p_vec[1 : N], main = expression("Sig:" ~ sigma ~ "= 1"), xlab = "P values")
lines(x, y, col = "dodgerblue", lwd = 3, lty = 2)
hist(p_vec[(N + 1) : (2 * N)], main = expression("Sig:" ~ sigma ~ "= 5"), xlab = "P values")
lines(x, y, col = "dodgerblue", lwd = 3, lty = 2)
hist(p_vec[(2 * N + 1) : (3 * N)], main = expression("Sig:" ~ sigma ~ "= 10"), xlab = "P values")
lines(x, y, col = "dodgerblue", lwd = 3, lty = 2)
hist(p_vec[(3 * N + 1) : (4 * N)], main = expression("Non:" ~ sigma ~ "= 1"), xlab = "P values")
lines(x, y, col = "dodgerblue", lwd = 3, lty = 2)
hist(p_vec[(4 * N + 1) : (5 * N)], main = expression("Non:" ~ sigma ~ "= 5"), xlab = "P values")
lines(x, y, col = "dodgerblue", lwd = 3, lty = 2)
hist(p_vec[(5 * N + 1) : (6 * N)], main = expression("Non:" ~ sigma ~ "= 10"), xlab = "P values")
lines(x, y, col = "dodgerblue", lwd = 3, lty = 2)
```

**Distribution of $R^2$ under null hypothesis**

As discussed on a [website](https://stats.stackexchange.com/questions/130069/what-is-the-distribution-of-r2-in-linear-regression-under-the-null-hypothesis), $R^2$ follows a **Beta** distribution under the null hypothesis.

```{r}
par(mfrow=c(2,3))
x = seq(0, 0.99, 0.01)

# get the distribution of R^2
dR2_to_F = function(R, p0 = p, n0 = N){
  a = (n0 - p0) / (p0 - 1)
  F = R * a / (1 - R)
  (F)
}

#summary(lm(f_vec ~ dR2_to_F(r_squared_vec, p, N)))

hist(r_squared_vec[1 : N], main = expression("Sig:" ~ sigma ~ "= 1"), xlab = expression(R^2 ~ " values"))
lines(x, dbeta(x, (p - 1) / 2, (n - p) / 2) * N / 20, col = "dodgerblue", lwd = 3, lty = 2)
hist(r_squared_vec[(N + 1) : (2 * N)], main = expression("Sig:" ~ sigma ~ "= 5"), xlab = expression(R^2 ~ " values"))
lines(x, dbeta(x, (p - 1) / 2, (n - p) / 2) * N / 20, col = "dodgerblue", lwd = 3, lty = 2)
hist(r_squared_vec[(2 * N + 1) : (3 * N)], main = expression("Sig:" ~ sigma ~ "= 10"), xlab = expression(R^2 ~ " values"))
lines(x, dbeta(x, (p - 1) / 2, (n - p) / 2) * N / 20, col = "dodgerblue", lwd = 3, lty = 2)
hist(r_squared_vec[(3 * N + 1) : (4 * N)], main = expression("Non:" ~ sigma ~ "= 1"), xlab = expression(R^2 ~ " values"))
lines(x, dbeta(x, (p - 1) / 2, (n - p) / 2) * N / 20, col = "dodgerblue", lwd = 3, lty = 2)
hist(r_squared_vec[(4 * N + 1) : (5 * N)], main = expression("Non:" ~ sigma ~ "= 5"), xlab = expression(R^2 ~ " values"))
lines(x, dbeta(x, (p - 1) / 2, (n - p) / 2) * N / 20, col = "dodgerblue", lwd = 3, lty = 2)
hist(r_squared_vec[(5 * N + 1) : (6 * N)], main = expression("Non:" ~ sigma ~ "= 10"), xlab = expression(R^2 ~ " values"))
lines(x, dbeta(x, (p - 1) / 2, (n - p) / 2) * N / 20, col = "dodgerblue", lwd = 3, lty = 2)

```

The **Beta** distribution that I find from the online forum matches the **non-significant** model with $\sigma = 1$. Based on the setup, we know that the **non-significant** model with $\sigma = 1$ should be very close to the true null distribution of $R^2$. Then we can conclude that the $R^2$ distribution has a left-skewed shape.

  - The $R^2$ distribution from the **significant** model with $\sigma = 1$ has a right-skewed shape. As $\sigma$ increases, the distribution approaches a left-skewed distribution.
  
  - The $R^2$ distribution from the **non-significant** model with $\sigma = 1$ has a left-skewed shape. As $\sigma$ increases, the distribution stays the similar left-skewed shape.

## Relationship between F, P value, and $R^2$ related to $\sigma$

### Significant model

F value decreases in general with an increasing $\sigma$

P value increases in general with an increasing $\sigma$

$R^2$ decreases in general with an increasing $\sigma$

### Non-significant model

F value stays the same in general with an increasing $\sigma$

P value stays the same in general with an increasing $\sigma$

$R^2$ value stays the same in general with an increasing $\sigma$


# Simulation Study 2: Using RMSE for Selection?

## Introduction

In homework we saw how Test RMSE can be used to select the “best” model. In this simulation study we will investigate how well this procedure works. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

Use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time you simulate the data, randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

Repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. For each value of $\sigma$, create a plot that shows how average Train RMSE and average Test RMSE changes as a function of model size. Also show the number of times the model of each size was chosen for each value of $\sigma$.

Done correctly, you will have simulated the $y$ vector $3×1000=3000$ times. You will have fit $9×3×1000=27000$ models. A minimal result would use $3$ plots. Additional plots may also be useful.

Potential discussions:

- Does the method **always** select the correct model? On average, does is select the correct model?
- How does the level of noise affect the results?

An additional tip:

- To address the second discussion topic, consider making a line graph for the RMSE values at each level of $\sigma$. Within a single plot for a given $\sigma$, one line could correspond to the training data and the other to the test data. 

## Methods

### define necessary functions

```{r}
# define a function that takes beta, sigma and X as input, and output a vector of y
sim_model = function(beta = c(0, 3, -4, 1.6, -1.1, 0.7, 0.5), sigma = 1, X = read.csv("study_2.csv")){
  X_m = as.matrix(X)
  X_m[, 1] = rep(1, length(X_m[, 1]))
  X[, 1] = X_m %*% beta + rnorm(length(X[, 1]), mean = 0, sd = sigma)
  (X)
}

# define a function to calculate RMSE
rmse = function(model, dataset){
  residuals = dataset$y - predict(model, newdata = dataset)
  sqrt(mean(sum(residuals ^ 2)))
}
```

### run simulations

```{r}
N = 1000
rmse_tr = matrix(as.matrix(rep(0, 9 * 3 * N)), nrow = N)
rmse_ts = matrix(as.matrix(rep(0, 9 * 3 * N)), nrow = N)
sigma_vec = c(1, 2, 4)
# since the last three predictors are not used in simulation, I put 0 in beta
beta_vec = c(0, 3, -4, 1.6, -1.1, 0.7, 0.5, 0, 0, 0)
x = read.csv("study_2.csv")

for(i in 1:N){
  index = 0
  for (sigma in sigma_vec){
    # get simulated data
    y = sim_model(beta = beta_vec, sigma = sigma, X = x)
    
    trn_idx = sample(1:nrow(y), nrow(y) / 2)
    # get the train and test data
    train = y[trn_idx, ]
    test = y[-trn_idx, ]
    
    model_1 = lm(y ~ x1, data = train)
    model_2 = lm(y ~ x1 + x2, data = train)
    model_3 = lm(y ~ x1 + x2 + x3, data = train)
    model_4 = lm(y ~ x1 + x2 + x3 + x4, data = train)
    model_5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = train)
    model_6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train)
    model_7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train)
    model_8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train)
    model_9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train)
    rmse_ts[i, 9 * index + 1] = rmse(model_1, test)
    rmse_ts[i, 9 * index + 2] = rmse(model_2, test)
    rmse_ts[i, 9 * index + 3] = rmse(model_3, test)
    rmse_ts[i, 9 * index + 4] = rmse(model_4, test)
    rmse_ts[i, 9 * index + 5] = rmse(model_5, test)
    rmse_ts[i, 9 * index + 6] = rmse(model_6, test)
    rmse_ts[i, 9 * index + 7] = rmse(model_7, test)
    rmse_ts[i, 9 * index + 8] = rmse(model_8, test)
    rmse_ts[i, 9 * index + 9] = rmse(model_9, test)
    rmse_tr[i, 9 * index + 1] = rmse(model_1, train)
    rmse_tr[i, 9 * index + 2] = rmse(model_2, train)
    rmse_tr[i, 9 * index + 3] = rmse(model_3, train)
    rmse_tr[i, 9 * index + 4] = rmse(model_4, train)
    rmse_tr[i, 9 * index + 5] = rmse(model_5, train)
    rmse_tr[i, 9 * index + 6] = rmse(model_6, train)
    rmse_tr[i, 9 * index + 7] = rmse(model_7, train)
    rmse_tr[i, 9 * index + 8] = rmse(model_8, train)
    rmse_tr[i, 9 * index + 9] = rmse(model_9, train)
    index = index + 1
  }
}
```

## Results

```{r}
# define a function to find the best model
find_min = function(m){
  result = rep(0, nrow(m))
  for (i in 1 : nrow(m)){
    result[i] = which.min(m[i,])
  }
  (result)
}

par(mfrow=c(1,3))
hist(find_min(rmse_ts[, 1 : 9]), breaks = (seq(1,9) + 0.5), 
     ylim = c(0, 600),
     main = expression("Best model, " ~ sigma ~ "= 1"),
     xlab = "Model #")

hist(find_min(rmse_ts[, 10 : 18]), breaks = (seq(1,9) + 0.5), 
     ylim = c(0, 600),
     main = expression("Best model, " ~ sigma ~ "= 2"),
     xlab = "Model #")

hist(find_min(rmse_ts[, 19 : 27]), breaks = (seq(1,9) + 0.5), 
     ylim = c(0, 600),
     main = expression("Best model, " ~ sigma ~ "= 4"),
     xlab = "Model #")
```

## Discussion

As displayed in the histogram, **model 6** overall have the smallest value of RMSE, which means it is the best model. Among 3 sigma groups, model 6 is the mostly selected model based on the value of RMSE. In addition, the result is as expected, because the true model that we use to generate the data has exactly 6 predictors. This means that **on average**, using RMSE criteria **we can select the correct model**. Another observation is that the model with more predictors are more likely to be selected thatn models with fewer predictors. This could mean that it is easier to over fit the data than under fit the data.

  
```{r}
# sigma = 1
result_1 = find_min(rmse_ts[, 1 : 9])
mean(result_1 == 6)

# sigma = 2
result_2 = find_min(rmse_ts[, 10 : 18])
mean(result_2 == 6)

# sigma = 4
result_4 = find_min(rmse_ts[, 19 : 27])
mean(result_4 == 6)
```
  - Based on RMSE we do not always get the correct model. Specifically, with  $\sigma = 1$ we can get the correct model in around `r mean(result_1 == 6)` of total simulations.
  
  - With increasing amount of noise, the proportion of the times that we get the correct model decreases from `r mean(result_1 == 6)` to `r mean(result_4 == 6)`.
  
  - In addition, with increasing amount of noise, the difference of RMSE from the test group and the train group becomes larger (figure below).
  
```{r}
par(mfrow=c(1,3))

plot(seq(1,9), colSums(rmse_ts[, 1:9]),
     xlab = "Model #",
     ylab = "Sum of RMSE",
     main = expression(" " ~ sigma ~ "= 1"))
lines(seq(1,9), colSums(rmse_ts[, 1:9]), col = "red", lty = 1)
lines(seq(1,9), colSums(rmse_tr[, 1:9]), col = "blue", lty = 2)
legend("topright", legend=c("Test", "Train"),
       col=c("red", "blue"), lty=1:2, cex=1.3)

plot(seq(1,9), colSums(rmse_ts[, 10:18]),
     xlab = "Model #",
     ylab = "Sum of RMSE",
     main = expression(" " ~ sigma ~ "= 2"),
     ylim = c(29000, 50000))
lines(seq(1,9), colSums(rmse_ts[, 10:18]), col = "red", lty = 1)
lines(seq(1,9), colSums(rmse_tr[, 10:18]), col = "blue", lty = 2)
legend("topright", legend=c("Test", "Train"),
       col=c("red", "blue"), lty=1:2, cex=1.3)

plot(seq(1,9), colSums(rmse_ts[, 19:27]),
     xlab = "Model #",
     ylab = "Sum of RMSE",
     main = expression(" " ~ sigma ~ "= 4"),
     ylim = c(61000, 74000))
lines(seq(1,9), colSums(rmse_ts[, 19:27]), col = "red", lty = 1)
lines(seq(1,9), colSums(rmse_tr[, 19:27]), col = "blue", lty = 2)
legend("topright", legend=c("Test", "Train"),
       col=c("red", "blue"), lty=1:2, cex=1.3)
```


# Simulation Study 3: Power

## Introduction

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

Use the following code to generate the predictor values, `x`: values for different sample sizes.

```{r eval=FALSE}
x_values = seq(0, 5, length = n)
```

For each possible $\beta_1$ and $\sigma$ combination, simulate from the true model at least $1000$ times. Each time, perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

It is *possible* to derive an expression for power mathematically, but often this is difficult, so instead, we rely on simulation.

Create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength, $\beta_1$.

Potential discussions:

- How do $n$, $\beta_1$, and $\sigma$ affect power? Consider additional plots to demonstrate these effects.
- Are $1000$ simulations sufficient?

An additional tip:

- Search online for examples of power curves to give you inspiration for how you might construct your own plots here. You'll find both two-sided and one-sided power curves. Based on the way you're asked to construct the $\beta_1$ vector, you should be able to figure out which type is appropriate here.

## Methods

### define necessary functions

```{r}
# define a function that takes beta, sigma and X as input, and output a vector of y
sim_model = function(beta, sigma, x){
  x_df = data.frame(y = rep(0, length(x)), x0 = rep(1, length(x)), x1 = x)
  x_m = as.matrix(x_df)
  x_df[, 1] = x_m[, -1] %*% beta + rnorm(length(x_df[, 1]), mean = 0, sd = sigma)
  (x_df)
}
```

### run simulations

```{r}
# initialization
beta_vec = seq(-2, 2, 0.1)
sigma_vec = c(1, 2, 4)
n_vec = c(10, 20, 30)
alpha = 0.05
p_m = matrix(as.matrix(rep(0, 3 * 3 * length(beta_vec))), nrow = length(beta_vec))
N = 1000

# for loop
for (beta in beta_vec){
  for (sigma in sigma_vec){
    for (n in n_vec){
      x_values = seq(0, 5, length = n)
      p_tmp = rep(0, N) # store P values for N simulations
      for (i in 1:N){
        y = sim_model(c(0, beta), sigma, x_values)
        model_temp = lm(y ~ x1, data = y)
        p_tmp[i] = summary(model_temp)$coefficients[2,4]
      }
      
      col_ind = (which(sigma_vec == sigma) - 1) * length(n_vec) + which(n_vec == n)
      # extract power from p_tmp
      p_m[which(beta_vec == beta), col_ind] = mean(p_tmp < alpha)
    }
  }
}
```

## Results

### Plots of Power vs $\beta_1$

```{r}
par(mfrow=c(3,3))
power = p_m[, 1]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 10, " ~ sigma ~ " = 1"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 2]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 20, " ~ sigma ~ " = 1"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 3]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 30, " ~ sigma ~ " = 1"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 4]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 10, " ~ sigma ~ " = 2"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 5]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 20, " ~ sigma ~ " = 2"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 6]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 30, " ~ sigma ~ " = 2"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 7]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 10, " ~ sigma ~ " = 4"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 8]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 20, " ~ sigma ~ " = 4"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

power = p_m[, 9]
plot(beta_vec, power, col = "darkorange", pch = 20,
     main = expression("n = 30, " ~ sigma ~ " = 4"),
     xlab = expression(" " ~ beta),
     ylim = c(0,1))

```

## Discussion

### How do $n, \beta_1$ and $\alpha$ affect power?

Based on our simulation results:

  - With all other two parameters set, the closer $\beta_1$ is to 0, the less the power
  
  - With all other two parameters set, the power increases with $n$
  
  - With all other two parameters set, the power decreases with $\sigma$
  
### Are 1000 simulations sufficient?

To verify this, I will calculate a specific setting of power with different number of simulations, where $n = 20$, $\beta = 0.5$, and $\sigma = 2$.

```{r}
N_vec = c(10, 20, 50, 80, 120, 150, 200, 250, 300, 400, 500, 700, 1000, 1200, 1500, 1700)
p_m = rep(0, length(N_vec))
for (N in N_vec){
  x_values = seq(0, 5, length = 20)
  p_tmp = rep(0, N)
  for (i in 1:N){
    y = sim_model(c(0, 0.5), 2, x_values)
    model_temp = lm(y ~ x1, data = y)
    p_tmp[i] = summary(model_temp)$coefficients[2,4]
  }
  p_m[which(N_vec == N)] = mean(p_tmp < alpha)
}

plot(N_vec, p_m, col = "darkorange", pch = 20,
     main = expression("n = 20, " ~ sigma ~ " = 2, " ~ beta ~ " = 0.5"),
     xlab = "N",
     ylab = "Power",
     ylim = c(0.1,0.6))
lines(N_vec, p_m)
```

From the plot above, we find that there is a large variation when N is less than 300, after which the power becomes stable. So, we believe 1000 simulations are likely enough.

***
